{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LeNet import LeNet\n",
    "from data_process import load_data,data_convert\n",
    "from evaluate import softmax,cal_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dir = \"./mnist_data/\"\n",
    "train_data_dir = \"train-images.idx3-ubyte\"\n",
    "train_label_dir = \"train-labels.idx1-ubyte\"\n",
    "test_data_dir = \"t10k-images.idx3-ubyte\"\n",
    "test_label_dir = \"t10k-labels.idx1-ubyte\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST data from files...\n",
      "./mnist_data/train-images.idx3-ubyte\n",
      "Load images from ./mnist_data/train-images.idx3-ubyte, number: 60000, data shape: (60000, 784)\n",
      "Load images from ./mnist_data/train-labels.idx1-ubyte, number: 60000, data shape: (60000, 1)\n",
      "Load images from ./mnist_data/t10k-images.idx3-ubyte, number: 10000, data shape: (10000, 784)\n",
      "Load images from ./mnist_data/t10k-labels.idx1-ubyte, number: 10000, data shape: (10000, 1)\n",
      "Got data. \n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels, test_images, test_labels = load_data(mnist_dir, train_data_dir, train_label_dir, test_data_dir, test_label_dir)\n",
    "print(\"Got data. \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_faults(fault_list):\n",
    "    for i in fault_list:\n",
    "        img = np.reshape(test_images[i, :], (28, 28))\n",
    "        label = np.argmax(test_images [i, :])\n",
    "        plt.matshow(img, cmap = plt.get_cmap('gray'))\n",
    "        plt.figure(figsize=(1,1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype(float)\n",
    "x,y = data_convert(train_images, train_labels,60000,10)\n",
    "x_val , y_val = data_convert(test_images,test_labels,10000,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(batch_size):\n",
    "\n",
    "    index = np.random.randint(0,len(x),batch_size)\n",
    "    return x[index],y.T[index].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y_pred,y):\n",
    "    batch_size ,_ = y_pred.shape\n",
    "    #y_pred = y_pred / y_pred.max(axis=1)[:,None] #防止溢出\n",
    "    #y_pred+=1e-5\n",
    "    y_pred = np.exp(y_pred)\n",
    "    y_sum = y_pred.sum(axis = 1)\n",
    "    y_pred = y_pred/y_sum[:,None]\n",
    "    loss = -np.log(y_pred).T * y\n",
    "    loss = loss.sum()/batch_size\n",
    "    grad = y_pred - y.T\n",
    "    grad /= batch_size\n",
    "    acc = (y_pred.argmax(axis=1) == y.argmax(axis=0)).mean()\n",
    "    return loss,grad,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(y_pred, y):\n",
    "    # y_pred: (N, C)\n",
    "    # y: (N, 1)\n",
    "    N = y_pred.shape[0]\n",
    "    ex = np.exp(y_pred)\n",
    "    sumx = np.sum(ex, axis=1)\n",
    "    loss = np.mean(np.log(sumx)-y_pred[range(N), list(y)])\n",
    "    grad = ex/sumx.reshape(N, 1)\n",
    "    grad[range(N), list(y)] -= 1\n",
    "    grad /= N\n",
    "    acc = np.mean(np.argmax(ex/sumx.reshape(N, 1), axis=1) == y.reshape(1, y.shape[0]))\n",
    "    return loss, grad, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, model, lr=1e-3, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.model = model\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.params = None\n",
    "        self.grad = None\n",
    "\n",
    "    def step(self):\n",
    "        self.params = model.get_params()\n",
    "        self.grad = model.get_grad()\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in self.params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "            for g in self.grad:\n",
    "                self.v.append(np.zeros_like(g))\n",
    "            assert(len(self.m) == len(self.params))\n",
    "            assert(len(self.v) == len(self.grad))\n",
    "\n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2 ** self.iter) / (1.0 - self.beta1 ** self.iter)\n",
    "\n",
    "        for i in range(len(self.params)):\n",
    "            self.m[i] += (1 - self.beta1) * (self.grad[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (self.grad[i] ** 2 - self.v[i])\n",
    "            self.params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AdaGrad import AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 234/234 [07:03<00:00,  1.81s/it, acc=0.918, loss=0.326]\n",
      "100%|██████████████████████████████████████| 234/234 [07:06<00:00,  1.82s/it, acc=0.953, loss=0.171]\n",
      "100%|██████████████████████████████████████| 234/234 [06:40<00:00,  1.71s/it, acc=0.961, loss=0.113]\n",
      "100%|██████████████████████████████████████| 234/234 [06:50<00:00,  1.75s/it, acc=0.953, loss=0.148]\n",
      " 57%|█████████████████████▌                | 133/234 [03:51<03:28,  2.07s/it, acc=0.941, loss=0.188]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "\n",
    "model = LeNet()\n",
    "optimizer = AdaGrad(model,1e-3)\n",
    "for e in range(10):\n",
    "    pbar = tqdm(range(0, int(x.shape[0]/batch_size)), ncols=100)\n",
    "    for i in pbar:\n",
    "        X_train,y_train = shuffle_batch(batch_size)\n",
    "        y_pred = model.fit(X_train,batch_size)\n",
    "\n",
    "        loss, grad, acc = softmax(y_pred, y_train)\n",
    "        model.back_prop(grad)\n",
    "        model.update()\n",
    "        #optimizer.update()\n",
    "        pbar.set_postfix(loss=loss, acc=acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▉                                      | 11/234 [00:19<06:43,  1.81s/it, acc=0.328, loss=1.92]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m loss, grad, acc \u001b[38;5;241m=\u001b[39m softmax(y_pred, y_train)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#loss, grad, acc = softmax_loss(y_pred, y_train.argmax(axis=0))\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mback_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#model.update(0.001)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\CODE\\machine-learning\\LeNet5\\LeNet.py:42\u001b[0m, in \u001b[0;36mLeNet.back_prop\u001b[1;34m(self, grad)\u001b[0m\n\u001b[0;32m     40\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPool1\u001b[38;5;241m.\u001b[39mbackprop(grad)\n\u001b[0;32m     41\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRelu1\u001b[38;5;241m.\u001b[39mbackprop(grad)\n\u001b[1;32m---> 42\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\CODE\\machine-learning\\LeNet5\\layers.py:84\u001b[0m, in \u001b[0;36mConv.backprop\u001b[1;34m(self, grad)\u001b[0m\n\u001b[0;32m     82\u001b[0m reverse_kernel  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mflip(reverse_kernel,axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m#卷积层输入的梯度实际上是输出的梯度经过padding后，与180°翻转的卷积核进行卷积的结果\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m grad_next \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreverse_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_size\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mBias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m#计算卷积核的梯度\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWeight)\n",
      "File \u001b[1;32md:\\CODE\\machine-learning\\LeNet5\\layers.py:60\u001b[0m, in \u001b[0;36mConv.conv2d\u001b[1;34m(self, input, kernel, padding, Bias)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m#选取input的这一块区域，并进行reshape\u001b[39;00m\n\u001b[0;32m     59\u001b[0m input_region \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[:, :, h_start:h_end, w_start:w_end]\u001b[38;5;241m.\u001b[39mreshape((N, \u001b[38;5;241m1\u001b[39m, C, filter_size, filter_size))\n\u001b[1;32m---> 60\u001b[0m output_matrix[:, :, h, w] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_region\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#通过numpy矩阵运算进行卷积\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \u001b[38;5;66;03m#是否加上偏置参数Bias\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     output_matrix[:, :, h, w] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBias\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:2183\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2114\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[0;32m   2115\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2178\u001b[0m \n\u001b[0;32m   2179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 2183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2184\u001b[0m                     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[0;32m   2189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2190\u001b[0m         initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "\n",
    "model = LeNet()\n",
    "optimizer = Adam(model,1e-3)\n",
    "for e in range(10):\n",
    "    pbar = tqdm(range(0, int(x.shape[0]/batch_size)), ncols=100)\n",
    "    for i in pbar:\n",
    "        X_train,y_train = shuffle_batch(batch_size)\n",
    "        y_pred = model.fit(X_train,batch_size)\n",
    "\n",
    "        loss, grad, acc = softmax(y_pred, y_train)\n",
    "        #loss, grad, acc = softmax_loss(y_pred, y_train.argmax(axis=0))\n",
    "        model.back_prop(grad)\n",
    "        #model.update(0.001)\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix(loss=loss, acc=acc)\n",
    "\n",
    "# val_X = data[\"X_val\"]\n",
    "# val_y = data[\"y_val\"]\n",
    "# y_pred = model.forward(val_X)\n",
    "# y_pred = np.argmax(y_pred, axis=1)\n",
    "# acc = np.mean(y_pred == val_y.reshape(1, val_y.shape[0]))\n",
    "# if acc > best_acc:\n",
    "#     best_acc = acc\n",
    "#     best_weight = model.get_params()\n",
    "# pbar.set_postfix(val_acc=acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
